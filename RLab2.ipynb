{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ec81f3363230662bf6ba752ed2680ac",
     "grade": false,
     "grade_id": "cell-4db65228bbcc6394",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "###  Credit Data \n",
    "In this R Lab assignment, we will use credit data set to identify whether a customer is risky or not in terms of repaying the loan. The dataset is in csv format, we read the data with read.csv command in R.   \n",
    "\n",
    "Credit dataset includes the following variables:\n",
    "- **creditability**:  Credit risk, Good (1), Bad (0)  (This is our target variable.)\n",
    "- **balance**: No account: (1), None (No balance) (2), Some Balance (3), High Balance (4)\n",
    "- **credit_duration**: Duration of credit in months\n",
    "- **payment_status**: Payment Status of Previous Credit. Unknown (0), Some Problems (1), Paid Up (2), No Problems (in this bank) (3),  No Problems (in all banks) (4)\n",
    "- **credit_amount**: current amount of credit from other accounts\n",
    "- **wealth**: None (1),Below 100 (2), In between 100 and 999 (3), In between 1000 and 2000 (4), above 2000 (5)\n",
    "- **employment_length**: Employment Length: Below 1 year (including unemployed) (1), one to four years (2), four to seven years (3), seven to ten years (4), more than ten years (5)\n",
    "- **sex_marital**: Sex and marital status: Male Divorced/Single (1), Male Married/Widowed (2), Female  Married/Widowed  (3), Female Divorced/Single (4)\n",
    "- **age**: Age in years\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "253de8773a13a8efb87e02951c25b31e",
     "grade": false,
     "grade_id": "cell-e25fa14acf798548",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t1000 obs. of  9 variables:\n",
      " $ creditability    : Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 2 2 2 2 2 2 ...\n",
      " $ balance          : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 1 2 1 1 1 1 1 4 2 ...\n",
      " $ credit_duration  : int  18 9 12 12 12 10 8 6 18 24 ...\n",
      " $ payment_status   : Factor w/ 5 levels \"0\",\"1\",\"2\",\"3\",..: 5 5 3 5 5 5 5 5 5 3 ...\n",
      " $ credit_amount    : int  1049 2799 841 2122 2171 2241 3398 1361 1098 3758 ...\n",
      " $ wealth           : Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 2 1 1 1 1 1 1 3 ...\n",
      " $ employment_length: Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 2 3 4 3 3 2 4 2 1 1 ...\n",
      " $ sex_marital      : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 2 3 2 3 3 3 3 3 2 2 ...\n",
      " $ age              : int  21 36 23 39 38 48 39 40 65 23 ...\n"
     ]
    }
   ],
   "source": [
    "# call the libraries we will use in the lab assignment\n",
    "library(tidyverse)\n",
    "library(caret)\n",
    "library(dplyr)\n",
    "library(testthat)\n",
    "# import the credit.csv data and name it as  creditdata in R. Data has the column names in the first row, so set header=TRUE\n",
    "creditdata<-read.csv(\"credit.csv\", header = TRUE, sep = \",\")\n",
    "# as can be seen with str(creditdata), some factor variables are coded as integer. \n",
    "# names of the columns that will need to be entered as factor data\n",
    "columns<-c('creditability','balance','payment_status','wealth','employment_length','sex_marital')\n",
    "# use the lapply function to selected columns to declare as factor\n",
    "creditdata[,columns]<-lapply(creditdata[,columns], factor)\n",
    "\n",
    "# check the data structure\n",
    "str(creditdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e916dbad71c42a01d1c4b3fc40f7f71a",
     "grade": false,
     "grade_id": "cell-b654270bd364cb7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t1000 obs. of  9 variables:\n",
      " $ creditability    : Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 2 2 2 2 2 2 ...\n",
      " $ balance          : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 1 2 1 1 1 1 1 4 2 ...\n",
      " $ credit_duration  : int  18 9 12 12 12 10 8 6 18 24 ...\n",
      " $ payment_status   : Factor w/ 5 levels \"0\",\"1\",\"2\",\"3\",..: 5 5 3 5 5 5 5 5 5 3 ...\n",
      " $ credit_amount    : int  1049 2799 841 2122 2171 2241 3398 1361 1098 3758 ...\n",
      " $ wealth           : Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 2 1 1 1 1 1 1 3 ...\n",
      " $ employment_length: Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 2 3 4 3 3 2 4 2 1 1 ...\n",
      " $ sex_marital      : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 2 3 2 3 3 3 3 3 2 2 ...\n",
      " $ age              : int  21 36 23 39 38 48 39 40 65 23 ...\n"
     ]
    }
   ],
   "source": [
    "# First we split the data into training and test sets by using the caret package\n",
    "# Training set contaings 75% of the data while test test has the remaining \n",
    "# In cases where the fractional split does not give a whole number, you have the set your list function in createDataPartition function in caret package\n",
    "\n",
    "\n",
    "set.seed(4230) # for reproducibility\n",
    "index_row <- createDataPartition(creditdata$creditability , p = 0.75,\n",
    "list = FALSE)   # get the row indices to be used for the training set\n",
    "\n",
    "train_credit <- creditdata[index_row, ]  # training data\n",
    "test_credit <- creditdata[-index_row, ]   #test data\n",
    "\n",
    "str(creditdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c175aaea8d6efb90142e3f9a7a0c1f74",
     "grade": false,
     "grade_id": "cell-b0dc7ea49eddbf3e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Logistic Regression without cross-validation  glm () function\n",
    "\n",
    "\n",
    "In logistic regression, the linear combinations of predictors will give us the log of odds rotio. \n",
    "$$ g(p) = \\log \\frac{p}{1-p}$$. Odds ratio  tells us how much more likely something will happen compared to not happening. \n",
    "\n",
    "We will first look at the logistic regression results without cross-validation. In the next section, we will get the results with cross validation. \n",
    "\n",
    "We will work on the following three models:\n",
    "- **Model1**:  Predictor: **age**\n",
    "- **Model2**:  Predictors: **age**, **credit_amount** , **credit_duration**  \n",
    "- **Model3**:  Predictors: all of them  (balance,credit_duration, payment_status,credit_amount,wealth, employment_length, sex_marital, age)\n",
    "\n",
    "As shown below, probability of an event will be less than 0.5 when log odds ratio is negative. Likewise, in cases where log odds ratio is positive, the probability becomes higher than 0.5. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce1ae1f1dbbfa56848aaadc5dbc90963",
     "grade": false,
     "grade_id": "cell-0c773c65c432697d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAADAFBMVEUAAAABAQECAgIDAwME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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      },
      "text/plain": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "  # logit vs probability\n",
    "p <- seq(0.01,.99,len=200)\n",
    "qplot(log( p/(1-p) ),p,  geom=\"line\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5930213892f3d95c3ca403081f0af606",
     "grade": false,
     "grade_id": "cell-d3ddf287f1847395",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 1: \n",
    "\n",
    "- By using *train_credit* data, use glm() function in R to estimate Model1 and call your model as **model1**\n",
    "- By using *train_credit* data, use glm() function in R to estimate Model2 and call your model as **model2**\n",
    "- By using *train_credit* data, use glm() function in R to estimate Model3 and call your model as **model3**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7d1586a72a9aa9c0359f0c0a9ba34c2",
     "grade": false,
     "grade_id": "cell-c799ea359d0a418c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = creditability ~ age, family = \"binomial\", data = train_credit)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-2.0032  -1.4040   0.7789   0.8862   1.0144  \n",
       "\n",
       "Coefficients:\n",
       "             Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept) -0.214581   0.279993  -0.766  0.44345    \n",
       "age          0.030539   0.007898   3.867  0.00011 ***\n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 916.30  on 749  degrees of freedom\n",
       "Residual deviance: 899.98  on 748  degrees of freedom\n",
       "AIC: 903.98\n",
       "\n",
       "Number of Fisher Scoring iterations: 4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = creditability ~ (age + credit_amount + credit_duration), \n",
       "    family = \"binomial\", data = train_credit)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-2.0615  -1.2223   0.7007   0.8605   1.6237  \n",
       "\n",
       "Coefficients:\n",
       "                  Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept)      5.165e-01  3.195e-01   1.617 0.105921    \n",
       "age              3.216e-02  8.224e-03   3.910 9.24e-05 ***\n",
       "credit_amount   -3.918e-05  3.559e-05  -1.101 0.270971    \n",
       "credit_duration -3.002e-02  8.572e-03  -3.502 0.000462 ***\n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 916.30  on 749  degrees of freedom\n",
       "Residual deviance: 868.28  on 746  degrees of freedom\n",
       "AIC: 876.28\n",
       "\n",
       "Number of Fisher Scoring iterations: 4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = creditability ~ (.), family = \"binomial\", data = train_credit)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-2.5918  -0.9060   0.4563   0.7743   1.9286  \n",
       "\n",
       "Coefficients:\n",
       "                     Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept)        -1.074e+00  8.316e-01  -1.292  0.19641    \n",
       "balance2            4.518e-01  2.262e-01   1.997  0.04580 *  \n",
       "balance3            9.373e-01  3.969e-01   2.362  0.01820 *  \n",
       "balance4            1.732e+00  2.490e-01   6.958 3.46e-12 ***\n",
       "credit_duration    -3.162e-02  9.402e-03  -3.363  0.00077 ***\n",
       "payment_status1    -4.058e-01  5.904e-01  -0.687  0.49191    \n",
       "payment_status2     7.331e-01  4.687e-01   1.564  0.11779    \n",
       "payment_status3     7.562e-01  5.406e-01   1.399  0.16189    \n",
       "payment_status4     1.200e+00  4.943e-01   2.428  0.01519 *  \n",
       "credit_amount      -5.024e-05  3.971e-05  -1.265  0.20585    \n",
       "wealth2             4.406e-02  2.941e-01   0.150  0.88090    \n",
       "wealth3             3.655e-01  4.645e-01   0.787  0.43138    \n",
       "wealth4             6.209e-01  5.090e-01   1.220  0.22258    \n",
       "wealth5             4.718e-01  2.712e-01   1.740  0.08194 .  \n",
       "employment_length2 -3.219e-01  4.161e-01  -0.774  0.43921    \n",
       "employment_length3  4.503e-02  3.883e-01   0.116  0.90767    \n",
       "employment_length4  4.601e-01  4.226e-01   1.089  0.27630    \n",
       "employment_length5  6.460e-02  3.998e-01   0.162  0.87163    \n",
       "sex_marital2        1.879e-01  4.018e-01   0.468  0.64011    \n",
       "sex_marital3        4.089e-01  3.872e-01   1.056  0.29101    \n",
       "sex_marital4        2.537e-01  4.754e-01   0.534  0.59360    \n",
       "age                 2.313e-02  9.837e-03   2.352  0.01868 *  \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 916.30  on 749  degrees of freedom\n",
       "Residual deviance: 746.09  on 728  degrees of freedom\n",
       "AIC: 790.09\n",
       "\n",
       "Number of Fisher Scoring iterations: 5\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exercise #1: \n",
    "#By using train_credit data, use glm() function to estimate Model1 and call your model as model1\n",
    "#By using train_credit data, use glm() function to estimate Model2 and call your model as model2\n",
    "#By using train_credit data, use glm() function to estimate Model3 and call your model as model3\n",
    "\n",
    "set.seed(4230)\n",
    "model1 <- glm(creditability ~ age,  family = \"binomial\", data = train_credit)\n",
    "model2 <- glm(f, family = \"binomial\", data = train_credit)\n",
    "model3 <- glm(creditability ~ (.), family = \"binomial\", data = train_credit)\n",
    "# your code here\n",
    "\n",
    "\n",
    "\n",
    "summary(model1)\n",
    "summary(model2)\n",
    "summary(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64f38864d0035d912c9e208d489f60a7",
     "grade": true,
     "grade_id": "cell-96fba102918d1aa5",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Passed!\"\n"
     ]
    }
   ],
   "source": [
    "# Test your code in here\n",
    "### BEGIN HIDDEN TEST\n",
    "\n",
    "test_that(\"Check number of dimensions\", {\n",
    "    expect_equal( summary(model1)[5][[1]],903.983962332741)})\n",
    "          \n",
    "          \n",
    "test_that(\"Check number of dimensions\", {\n",
    "    expect_equal( summary(model2)[5][[1]],876.276062513887)})\n",
    "\n",
    "test_that(\"Check number of dimensions\", {\n",
    "    expect_equal( summary(model3)[5][[1]],790.094608058906)})\n",
    "print(\"Passed!\")\n",
    "\n",
    "### END HIDDEN TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd82a0483209806e1632d84d3d4005e3",
     "grade": false,
     "grade_id": "cell-176180b387062dea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 2:\n",
    "\n",
    "Given that our target variable is a binary indicator, we can calculate the logit numbers (natural logarithm of the odds ratio) from the logistic regression output. \n",
    "- Based on **model1**, calculate the logit valus in **train_credit** data and call it **model1_logit**. \n",
    "- Based on **model2**, calculate the logit valus in **train_credit** data and call it **model2_logit**. \n",
    "- Based on **model3**, calculate the logit valus in **train_credit** data and call it **model3_logit**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'creditability'</li><li>'balance'</li><li>'credit_duration'</li><li>'payment_status'</li><li>'credit_amount'</li><li>'wealth'</li><li>'employment_length'</li><li>'sex_marital'</li><li>'age'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'creditability'\n",
       "\\item 'balance'\n",
       "\\item 'credit\\_duration'\n",
       "\\item 'payment\\_status'\n",
       "\\item 'credit\\_amount'\n",
       "\\item 'wealth'\n",
       "\\item 'employment\\_length'\n",
       "\\item 'sex\\_marital'\n",
       "\\item 'age'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'creditability'\n",
       "2. 'balance'\n",
       "3. 'credit_duration'\n",
       "4. 'payment_status'\n",
       "5. 'credit_amount'\n",
       "6. 'wealth'\n",
       "7. 'employment_length'\n",
       "8. 'sex_marital'\n",
       "9. 'age'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"creditability\"     \"balance\"           \"credit_duration\"  \n",
       "[4] \"payment_status\"    \"credit_amount\"     \"wealth\"           \n",
       "[7] \"employment_length\" \"sex_marital\"       \"age\"              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "names(train_credit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce66406ac979ab9a7f183672e65ea609",
     "grade": false,
     "grade_id": "cell-b118fa6e96e34670",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "654.015847229984"
      ],
      "text/latex": [
       "654.015847229984"
      ],
      "text/markdown": [
       "654.015847229984"
      ],
      "text/plain": [
       "[1] 654.0158"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "680.710224711692"
      ],
      "text/latex": [
       "680.710224711692"
      ],
      "text/markdown": [
       "680.710224711692"
      ],
      "text/plain": [
       "[1] 680.7102"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "825.41761904404"
      ],
      "text/latex": [
       "825.41761904404"
      ],
      "text/markdown": [
       "825.41761904404"
      ],
      "text/plain": [
       "[1] 825.4176"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exercise #2: \n",
    "\n",
    "# Based on **model1**, calculate the logit valus in **train_credit** data and call it **model1_logit**. \n",
    "# Based on **model2**, calculate the logit valus in **train_credit** data and call it **model2_logit**. \n",
    "# Based on **model3**, calculate the logit valus in **train_credit** data and call it **model3_logit**. \n",
    "\n",
    "set.seed(4230)\n",
    "\n",
    "# your code here\n",
    "model1_logit = predict(model1, type = \"link\")\n",
    "model2_logit = predict(model2, type = \"link\")\n",
    "model3_logit = predict(model3, type = \"link\")\n",
    "\n",
    "sum(model1_logit)\n",
    "sum(model2_logit)\n",
    "sum(model3_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea14e1aadd6ca33281b25285fd3788d1",
     "grade": true,
     "grade_id": "cell-f58472d62b399a22",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Passed!\"\n"
     ]
    }
   ],
   "source": [
    "# Test your code in here\n",
    "### BEGIN HIDDEN TEST\n",
    "\n",
    "test_that(\"Check the accuracy of logit numbers\", {\n",
    "    expect_equal( sum(model1_logit),654.015847229984)})\n",
    "\n",
    "test_that(\"Check the accuracy of logit numbers\", {\n",
    "    expect_equal( sum(model2_logit),680.710224711692)})\n",
    "\n",
    "test_that(\"Check the accuracy of logit numbers\", {\n",
    "    expect_equal( sum(model3_logit),825.41761904404)})\n",
    "\n",
    "print(\"Passed!\")\n",
    "\n",
    "### END HIDDEN TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d2b227a1e7c2698d6645ef61d8aa278",
     "grade": false,
     "grade_id": "cell-fc2cb7aca92e466d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 3:\n",
    "\n",
    "Now, it is time to calculate the probability that a customer, given the predictors, has GOOD credit risk (creditability takes the value of 1) from the logistic regression results of model1, model1 and model3. \n",
    "\n",
    "-  Calculate the probabilities of  good credit risk for the customers in **train_credit** data based on **model1** and call it **model1_prob**\n",
    "\n",
    "-  Calculate the probabilities of  good credit risk for the customers in **train_credit** data based on **model2** and call it **model2_prob**\n",
    "\n",
    "-  Calculate the probabilities of  good credit risk for the customers in **train_credit** data based on **model3** and call it **model3_prob**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8fc3cdec41fa02de42c8815ac305a62e",
     "grade": false,
     "grade_id": "cell-a964c5d86b7e326d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1         3         5         6         7         8         9        10 \n",
      "0.6050922 0.6195893 0.7202866 0.7775195 0.7263977 0.7324249 0.8545104 0.6195893 \n",
      "       11        12        13        14        16        17        19        20 \n",
      "0.7078173 0.6267605 0.6752690 0.6752690 0.7556755 0.7324249 0.7078173 0.7263977 \n",
      "       21        22        24        26        27        29        30        32 \n",
      "0.7140927 0.7827573 0.6409342 0.7929639 0.6267605 0.6617353 0.8169139 0.7140927 \n",
      "       33        35        36        38        39        40        41        43 \n",
      "0.6950274 0.7078173 0.7263977 0.6409342 0.6752690 0.7383672 0.6195893 0.7078173 \n",
      "       44        45        46        47        48        49        50        52 \n",
      "0.6950274 0.8386571 0.6195893 0.6338763 0.6685370 0.6885161 0.6409342 0.6195893 \n",
      "       53        54        55        56        57        58        59        60 \n",
      "0.6685370 0.7827573 0.7383672 0.8258728 0.8467516 0.6479316 0.7383672 0.7078173 \n",
      "       61        62        64        65        69        70        72        73 \n",
      "0.7556755 0.6617353 0.7612697 0.7499933 0.6123656 0.8344821 0.8258728 0.7263977 \n",
      "       74        75        78        79        80        82        83        84 \n",
      "0.6752690 0.7140927 0.8344821 0.6685370 0.7014617 0.7929639 0.6685370 0.7078173 \n",
      "       85        86        88        89        91        92        93        95 \n",
      "0.8467516 0.6752690 0.6950274 0.7263977 0.6479316 0.6338763 0.6548661 0.7383672 \n",
      "       96        98        99       100       102       103       104       105 \n",
      "0.6819293 0.6685370 0.6338763 0.7383672 0.6123656 0.6950274 0.8076016 0.6885161 \n",
      "      106       107       108       109       112       116       118       120 \n",
      "0.6752690 0.6617353 0.7263977 0.7202866 0.7827573 0.6548661 0.6548661 0.6819293 \n",
      "      122       123       124       126       127       128       130       131 \n",
      "0.7014617 0.6685370 0.7499933 0.7721921 0.6267605 0.6819293 0.7078173 0.6409342 \n",
      "      133       134       136       137       138       139       140       141 \n",
      "0.7383672 0.6409342 0.6885161 0.8076016 0.6819293 0.7263977 0.6752690 0.7612697 \n",
      "      142       143       144       145       149       150       152       153 \n",
      "0.8506726 0.7442236 0.6752690 0.6338763 0.7078173 0.7775195 0.6617353 0.7014617 \n",
      "      155       156       157       158       159       161       162       163 \n",
      "0.6409342 0.7014617 0.6195893 0.7202866 0.8302208 0.7014617 0.7078173 0.6338763 \n",
      "      164       165       166       168       169       170       171       174 \n",
      "0.6548661 0.6195893 0.7775195 0.8854689 0.6267605 0.6548661 0.6267605 0.6819293 \n",
      "      175       176       177       178       179       180       181       182 \n",
      "0.6752690 0.7078173 0.6479316 0.7556755 0.6950274 0.7140927 0.6195893 0.6685370 \n",
      "      184       185       186       187       188       189       190       191 \n",
      "0.6885161 0.6479316 0.6752690 0.7879055 0.6752690 0.6819293 0.6548661 0.6617353 \n",
      "      193       194       195       196       199       200       201       202 \n",
      "0.6267605 0.8076016 0.6338763 0.7014617 0.7014617 0.6479316 0.6685370 0.7324249 \n",
      "      204       206       207       208       209       210       211       214 \n",
      "0.6685370 0.6885161 0.5977719 0.6195893 0.7078173 0.7014617 0.6617353 0.8854689 \n",
      "      215       216       217       219       220       221       223       224 \n",
      "0.7014617 0.7140927 0.6752690 0.7014617 0.7612697 0.7078173 0.7014617 0.6479316 \n",
      "      225       227       228       229       230       231       233       234 \n",
      "0.6548661 0.7014617 0.6479316 0.7078173 0.6685370 0.7078173 0.7612697 0.7324249 \n",
      "      235       236       237       238       239       240       241       242 \n",
      "0.6338763 0.8123022 0.7879055 0.8506726 0.6195893 0.6267605 0.7721921 0.7442236 \n",
      "      243       246       247       248       250       251       252       253 \n",
      "0.6267605 0.6195893 0.6267605 0.7014617 0.6409342 0.7499933 0.7556755 0.7202866 \n",
      "      254       255       256       257       258       259       260       262 \n",
      "0.6548661 0.7202866 0.6409342 0.6479316 0.6195893 0.7979327 0.8854689 0.7014617 \n",
      "      263       264       265       267       268       269       271       272 \n",
      "0.6195893 0.5977719 0.6950274 0.7140927 0.7078173 0.6123656 0.7879055 0.7324249 \n",
      "      276       278       280       281       282       283       284       285 \n",
      "0.7014617 0.6479316 0.7827573 0.6617353 0.7667754 0.5977719 0.7721921 0.6617353 \n",
      "      286       287       288       289       290       291       292       294 \n",
      "0.8076016 0.7979327 0.6819293 0.7929639 0.6885161 0.7140927 0.5977719 0.6752690 \n",
      "      296       298       301       303       304       307       308       309 \n",
      "0.6685370 0.6950274 0.7612697 0.6409342 0.6885161 0.7078173 0.6409342 0.6548661 \n",
      "      311       313       314       315       316       317       318       319 \n",
      "0.8545104 0.7383672 0.6409342 0.7383672 0.7612697 0.7324249 0.6267605 0.6752690 \n",
      "      322       323       324       325       326       327       329       330 \n",
      "0.6548661 0.6548661 0.6479316 0.6752690 0.7140927 0.7014617 0.6819293 0.6885161 \n",
      "      333       334       335       337       338       339       340       341 \n",
      "0.6267605 0.6338763 0.8076016 0.7775195 0.7827573 0.6819293 0.8123022 0.7324249 \n",
      "      343       345       346       347       348       349       350       351 \n",
      "0.7202866 0.7929639 0.7202866 0.7667754 0.8467516 0.6685370 0.6819293 0.6409342 \n",
      "      352       353       354       356       357       358       359       360 \n",
      "0.6950274 0.7667754 0.6617353 0.7442236 0.8467516 0.6819293 0.6195893 0.8302208 \n",
      "      361       362       363       364       366       368       369       370 \n",
      "0.8386571 0.6123656 0.6479316 0.6409342 0.6950274 0.6752690 0.6548661 0.6267605 \n",
      "      371       372       375       376       377       378       379       380 \n",
      "0.6050922 0.6950274 0.6950274 0.6479316 0.6548661 0.6685370 0.6123656 0.7078173 \n",
      "      381       382       383       384       385       386       387       389 \n",
      "0.6123656 0.6409342 0.8467516 0.6123656 0.7078173 0.6267605 0.6685370 0.7140927 \n",
      "      390       392       393       394       396       398       402       403 \n",
      "0.6409342 0.8582661 0.7879055 0.7014617 0.6338763 0.6685370 0.7263977 0.6195893 \n",
      "      404       407       409       410       413       414       415       416 \n",
      "0.6409342 0.6685370 0.7078173 0.6617353 0.7014617 0.7556755 0.7442236 0.6617353 \n",
      "      417       418       419       420       421       422       423       424 \n",
      "0.8214373 0.6479316 0.6617353 0.7556755 0.6548661 0.7324249 0.8655345 0.7979327 \n",
      "      425       426       428       430       431       432       433       434 \n",
      "0.7667754 0.7979327 0.6950274 0.6885161 0.6409342 0.8123022 0.6479316 0.6548661 \n",
      "      435       437       438       441       444       445       446       448 \n",
      "0.8214373 0.6752690 0.6267605 0.6195893 0.7324249 0.8169139 0.7879055 0.6050922 \n",
      "      449       452       453       455       457       461       462       463 \n",
      "0.6819293 0.7263977 0.8724856 0.6123656 0.6617353 0.8344821 0.7324249 0.6195893 \n",
      "      464       466       467       468       469       470       471       473 \n",
      "0.7612697 0.8123022 0.6685370 0.7014617 0.6267605 0.7721921 0.7721921 0.7140927 \n",
      "      476       477       478       479       480       481       482       483 \n",
      "0.6479316 0.7721921 0.6338763 0.8214373 0.8619405 0.7775195 0.7324249 0.6819293 \n",
      "      485       487       488       490       491       493       494       495 \n",
      "0.7014617 0.7014617 0.7140927 0.7612697 0.6885161 0.6752690 0.7667754 0.8076016 \n",
      "      497       499       501       503       504       505       507       508 \n",
      "0.6617353 0.7202866 0.6548661 0.7929639 0.6885161 0.7202866 0.7202866 0.7667754 \n",
      "      509       511       512       513       516       517       518       519 \n",
      "0.6885161 0.6267605 0.8169139 0.7140927 0.6617353 0.7078173 0.7140927 0.7556755 \n",
      "      520       521       523       524       525       526       528       530 \n",
      "0.6195893 0.6548661 0.6617353 0.6050922 0.6950274 0.6752690 0.6479316 0.6338763 \n",
      "      531       532       533       534       535       537       538       539 \n",
      "0.7442236 0.7499933 0.7556755 0.6819293 0.6479316 0.7140927 0.7442236 0.7667754 \n",
      "      540       541       543       544       546       547       548       549 \n",
      "0.6885161 0.6123656 0.7140927 0.6479316 0.6409342 0.6409342 0.6479316 0.6123656 \n",
      "      550       551       552       553       554       555       556       557 \n",
      "0.7140927 0.6685370 0.7827573 0.6479316 0.7324249 0.5977719 0.6409342 0.6685370 \n",
      "      558       559       560       561       562       563       564       567 \n",
      "0.6885161 0.6409342 0.7014617 0.7556755 0.6479316 0.6267605 0.7721921 0.6752690 \n",
      "      568       569       570       572       573       574       576       577 \n",
      "0.7979327 0.7499933 0.7140927 0.7014617 0.6685370 0.6950274 0.7775195 0.7442236 \n",
      "      579       580       581       583       584       585       587       588 \n",
      "0.6195893 0.6685370 0.6267605 0.6752690 0.6123656 0.8028118 0.7078173 0.7140927 \n",
      "      589       591       593       594       595       596       598       600 \n",
      "0.6338763 0.6479316 0.6195893 0.6617353 0.6819293 0.6548661 0.8545104 0.6617353 \n",
      "      602       603       606       607       608       609       610       611 \n",
      "0.6685370 0.8885297 0.6338763 0.7014617 0.7140927 0.6409342 0.6685370 0.6195893 \n",
      "      612       613       614       616       617       618       619       620 \n",
      "0.7442236 0.6685370 0.7667754 0.6617353 0.7202866 0.6685370 0.6752690 0.7721921 \n",
      "      621       623       624       625       626       627       628       629 \n",
      "0.7775195 0.8582661 0.6617353 0.8885297 0.7078173 0.7202866 0.7667754 0.7078173 \n",
      "      630       631       632       633       634       635       637       640 \n",
      "0.8123022 0.6338763 0.7612697 0.6548661 0.6685370 0.7263977 0.8344821 0.8467516 \n",
      "      641       643       646       647       648       649       650       651 \n",
      "0.7667754 0.7556755 0.7499933 0.7979327 0.6819293 0.7383672 0.6950274 0.7775195 \n",
      "      652       653       654       655       656       657       658       659 \n",
      "0.7202866 0.7202866 0.8028118 0.7202866 0.7442236 0.7202866 0.8506726 0.7014617 \n",
      "      660       661       662       664       666       669       670       671 \n",
      "0.6409342 0.6885161 0.6617353 0.6548661 0.7827573 0.6338763 0.6950274 0.6195893 \n",
      "      672       674       676       677       678       680       681       682 \n",
      "0.6819293 0.7556755 0.6548661 0.6338763 0.7775195 0.6548661 0.6195893 0.7324249 \n",
      "      684       685       687       689       690       691       692       693 \n",
      "0.7879055 0.6267605 0.6752690 0.7612697 0.6819293 0.6050922 0.6479316 0.6123656 \n",
      "      694       697       698       699       701       702       703       704 \n",
      "0.7667754 0.7078173 0.6548661 0.6479316 0.7721921 0.6950274 0.6195893 0.8214373 \n",
      "      705       707       708       709       711       712       713       714 \n",
      "0.8619405 0.5977719 0.7078173 0.7140927 0.8427467 0.8545104 0.7078173 0.7383672 \n",
      "      717       718       719       720       721       724       726       727 \n",
      "0.7879055 0.8076016 0.6479316 0.6267605 0.7879055 0.6752690 0.6050922 0.6752690 \n",
      "      728       730       731       732       733       734       735       737 \n",
      "0.7263977 0.6479316 0.8123022 0.6950274 0.6195893 0.6195893 0.7442236 0.7140927 \n",
      "      738       739       740       741       742       743       744       745 \n",
      "0.7667754 0.7202866 0.6752690 0.6752690 0.8386571 0.5977719 0.8467516 0.6267605 \n",
      "      746       747       748       750       751       752       753       754 \n",
      "0.6050922 0.7078173 0.7556755 0.7929639 0.6685370 0.6548661 0.6819293 0.6548661 \n",
      "      755       756       757       759       760       761       763       766 \n",
      "0.6195893 0.6885161 0.8344821 0.6267605 0.6479316 0.6548661 0.6617353 0.6123656 \n",
      "      767       769       770       771       772       773       774       775 \n",
      "0.6338763 0.6617353 0.6267605 0.6685370 0.6479316 0.7721921 0.6267605 0.6267605 \n",
      "      776       778       779       780       781       782       784       785 \n",
      "0.6950274 0.7556755 0.6885161 0.7324249 0.7827573 0.7775195 0.6267605 0.6617353 \n",
      "      787       788       789       791       792       797       799       800 \n",
      "0.6819293 0.8386571 0.7078173 0.6409342 0.6338763 0.6950274 0.6617353 0.7202866 \n",
      "      801       802       803       804       806       807       809       810 \n",
      "0.7014617 0.6819293 0.6338763 0.6885161 0.6267605 0.6548661 0.7263977 0.7014617 \n",
      "      811       812       813       814       817       818       819       820 \n",
      "0.6195893 0.7078173 0.6195893 0.6267605 0.7775195 0.6409342 0.6267605 0.5904078 \n",
      "      821       822       823       824       825       826       827       828 \n",
      "0.6479316 0.6617353 0.6819293 0.6338763 0.6617353 0.7721921 0.6950274 0.7499933 \n",
      "      829       830       832       833       834       835       836       838 \n",
      "0.6685370 0.6123656 0.6409342 0.6479316 0.6409342 0.6819293 0.6195893 0.7499933 \n",
      "      839       842       843       844       846       847       848       849 \n",
      "0.7263977 0.6752690 0.6617353 0.6617353 0.6195893 0.6685370 0.8344821 0.7879055 \n",
      "      850       853       854       855       856       857       858       859 \n",
      "0.7721921 0.6752690 0.7140927 0.7499933 0.7667754 0.6685370 0.6752690 0.6950274 \n",
      "      860       861       863       866       867       870       871       872 \n",
      "0.6885161 0.7202866 0.6195893 0.7263977 0.6338763 0.6338763 0.8076016 0.6338763 \n",
      "      874       875       877       879       880       881       882       884 \n",
      "0.6548661 0.6950274 0.6548661 0.6123656 0.7324249 0.6338763 0.6950274 0.6338763 \n",
      "      886       887       890       891       892       893       894       895 \n",
      "0.6267605 0.6479316 0.6617353 0.6479316 0.6267605 0.6338763 0.6885161 0.6950274 \n",
      "      896       897       898       900       901       904       906       907 \n",
      "0.7202866 0.7383672 0.6050922 0.6479316 0.6752690 0.7442236 0.6267605 0.7556755 \n",
      "      908       909       910       912       914       915       917       919 \n",
      "0.6195893 0.7499933 0.8028118 0.7014617 0.6752690 0.6195893 0.5977719 0.6267605 \n",
      "      920       922       923       924       926       928       929       930 \n",
      "0.7667754 0.7667754 0.6195893 0.7879055 0.5977719 0.6050922 0.7263977 0.6338763 \n",
      "      931       933       934       935       936       937       938       939 \n",
      "0.6685370 0.7667754 0.7078173 0.7499933 0.6885161 0.6617353 0.7078173 0.6195893 \n",
      "      940       941       942       943       944       946       947       949 \n",
      "0.7442236 0.7556755 0.7383672 0.7383672 0.7140927 0.6950274 0.6338763 0.6819293 \n",
      "      950       952       954       955       956       957       958       959 \n",
      "0.6885161 0.6548661 0.6267605 0.6409342 0.8123022 0.8386571 0.6050922 0.6409342 \n",
      "      961       962       963       965       967       968       969       970 \n",
      "0.8655345 0.6123656 0.8258728 0.7140927 0.6479316 0.6409342 0.6195893 0.7324249 \n",
      "      971       973       974       975       976       977       978       980 \n",
      "0.6885161 0.7140927 0.6617353 0.6195893 0.6950274 0.6819293 0.8655345 0.7014617 \n",
      "      982       983       984       985       986       987       988       989 \n",
      "0.6548661 0.6685370 0.6338763 0.6950274 0.7442236 0.7612697 0.5977719 0.6195893 \n",
      "      990       992       996       998       999      1000 \n",
      "0.5977719 0.7556755 0.6050922 0.6685370 0.7979327 0.6752690 \n",
      "        1         3         5         6         7         8         9        10 \n",
      "0.6480198 0.7032698 0.7846776 0.8418399 0.8017544 0.8276827 0.8832165 0.5959005 \n",
      "       11        12        13        14        16        17        19        20 \n",
      "0.7669041 0.5362597 0.7784312 0.4440826 0.8385570 0.7888820 0.6229000 0.7608217 \n",
      "       21        22        24        26        27        29        30        32 \n",
      "0.7993331 0.8333428 0.7184006 0.8374410 0.7114582 0.7025404 0.7044392 0.6194221 \n",
      "       33        35        36        38        39        40        41        43 \n",
      "0.5670938 0.7606781 0.8258887 0.7143409 0.6986026 0.7237009 0.7020010 0.7490392 \n",
      "       44        45        46        47        48        49        50        52 \n",
      "0.6853936 0.8686534 0.6610807 0.6342902 0.7042902 0.6898556 0.6405987 0.5054701 \n",
      "       53        54        55        56        57        58        59        60 \n",
      "0.7618871 0.8376130 0.7041430 0.8814474 0.8925664 0.7095451 0.7466241 0.6160477 \n",
      "       61        62        64        65        69        70        72        73 \n",
      "0.8213554 0.6510404 0.6122486 0.8155240 0.6904704 0.8383185 0.8034206 0.6456173 \n",
      "       74        75        78        79        80        82        83        84 \n",
      "0.6805903 0.7708067 0.8388064 0.6874062 0.7528997 0.7200739 0.6706273 0.7560909 \n",
      "       85        86        88        89        91        92        93        95 \n",
      "0.8841634 0.7330336 0.5877392 0.8269789 0.7203758 0.5750927 0.5868448 0.7398784 \n",
      "       96        98        99       100       102       103       104       105 \n",
      "0.6359118 0.5562528 0.7244310 0.5843932 0.6774499 0.6692385 0.7070413 0.6866867 \n",
      "      106       107       108       109       112       116       118       120 \n",
      "0.6766257 0.7375894 0.7636013 0.7227028 0.8693519 0.6527791 0.6443828 0.6693121 \n",
      "      122       123       124       126       127       128       130       131 \n",
      "0.5823441 0.7721057 0.8008715 0.7516516 0.6907704 0.7770751 0.6987052 0.6356864 \n",
      "      133       134       136       137       138       139       140       141 \n",
      "0.8280498 0.6803311 0.7920088 0.8630712 0.6756713 0.8254827 0.6614027 0.6753719 \n",
      "      142       143       144       145       149       150       152       153 \n",
      "0.8960559 0.7205939 0.7593101 0.5364197 0.7778998 0.7809535 0.7317935 0.4635699 \n",
      "      155       156       157       158       159       161       162       163 \n",
      "0.7205040 0.6934610 0.6978704 0.6467064 0.6729181 0.7740194 0.7117105 0.6092924 \n",
      "      164       165       166       168       169       170       171       174 \n",
      "0.6975407 0.6879283 0.8463418 0.8806113 0.6745282 0.7654166 0.4946160 0.6822722 \n",
      "      175       176       177       178       179       180       181       182 \n",
      "0.7457743 0.6957029 0.6229542 0.8176409 0.7296925 0.4417387 0.7185940 0.7108481 \n",
      "      184       185       186       187       188       189       190       191 \n",
      "0.6378787 0.7422648 0.5517395 0.8416315 0.6589241 0.5928508 0.6619412 0.6579328 \n",
      "      193       194       195       196       199       200       201       202 \n",
      "0.3810368 0.8090892 0.7131502 0.5907775 0.7681415 0.4507658 0.6199213 0.6582889 \n",
      "      204       206       207       208       209       210       211       214 \n",
      "0.6125960 0.7033695 0.6804913 0.6790125 0.4394140 0.7797012 0.7808135 0.9349291 \n",
      "      215       216       217       219       220       221       223       224 \n",
      "0.7876965 0.7775354 0.7501644 0.4528933 0.8338125 0.7422632 0.6766001 0.5589176 \n",
      "      225       227       228       229       230       231       233       234 \n",
      "0.6840303 0.7170787 0.7176892 0.7275299 0.4751636 0.7116864 0.6121401 0.6794092 \n",
      "      235       236       237       238       239       240       241       242 \n",
      "0.7177522 0.8178430 0.8031480 0.8532847 0.7525891 0.5042174 0.8214163 0.4157616 \n",
      "      243       246       247       248       250       251       252       253 \n",
      "0.6651830 0.5984451 0.6631557 0.6811098 0.6263429 0.7764815 0.7591367 0.7935883 \n",
      "      254       255       256       257       258       259       260       262 \n",
      "0.5486828 0.6068287 0.6893334 0.6881975 0.7051259 0.8578691 0.9315546 0.7785386 \n",
      "      263       264       265       267       268       269       271       272 \n",
      "0.7121710 0.6980472 0.7669340 0.7946383 0.6359483 0.6930516 0.8694086 0.6037821 \n",
      "      276       278       280       281       282       283       284       285 \n",
      "0.7589042 0.6482240 0.8554298 0.7759379 0.8245152 0.6493642 0.7867145 0.6653235 \n",
      "      286       287       288       289       290       291       292       294 \n",
      "0.8855891 0.8802955 0.7630827 0.8566874 0.7444934 0.7696557 0.6507485 0.7818448 \n",
      "      296       298       301       303       304       307       308       309 \n",
      "0.5204568 0.7959865 0.8306154 0.6189822 0.7662113 0.8084799 0.7580038 0.6647147 \n",
      "      311       313       314       315       316       317       318       319 \n",
      "0.9011388 0.8382106 0.6422296 0.8360096 0.8118826 0.7352062 0.6920799 0.7329748 \n",
      "      322       323       324       325       326       327       329       330 \n",
      "0.7261996 0.7315972 0.7312544 0.7599809 0.7181677 0.7835049 0.7111259 0.6898668 \n",
      "      333       334       335       337       338       339       340       341 \n",
      "0.7402466 0.6478779 0.8182235 0.8506028 0.8521126 0.7565249 0.8663968 0.6860284 \n",
      "      343       345       346       347       348       349       350       351 \n",
      "0.7448963 0.8543939 0.7962226 0.8411736 0.9012781 0.7384360 0.7609322 0.7253804 \n",
      "      352       353       354       356       357       358       359       360 \n",
      "0.6962707 0.5636221 0.7214597 0.5928842 0.8464843 0.7680032 0.6545107 0.8539272 \n",
      "      361       362       363       364       366       368       369       370 \n",
      "0.8878125 0.6880285 0.7607438 0.7365295 0.7579513 0.7498599 0.7133313 0.6105065 \n",
      "      371       372       375       376       377       378       379       380 \n",
      "0.5533002 0.6874764 0.7144451 0.6083485 0.7336522 0.6075386 0.6713594 0.6246684 \n",
      "      381       382       383       384       385       386       387       389 \n",
      "0.6319521 0.6413350 0.5503901 0.7196958 0.5634081 0.7310847 0.7640551 0.7954476 \n",
      "      390       392       393       394       396       398       402       403 \n",
      "0.6276012 0.9020798 0.8702470 0.7565837 0.7316447 0.7489936 0.7973348 0.7355583 \n",
      "      404       407       409       410       413       414       415       416 \n",
      "0.6242163 0.7002409 0.7424655 0.4768891 0.7761707 0.7535899 0.8155314 0.6227022 \n",
      "      417       418       419       420       421       422       423       424 \n",
      "0.8780342 0.6075143 0.7032631 0.8058335 0.7361861 0.8054524 0.8981407 0.8512952 \n",
      "      425       426       428       430       431       432       433       434 \n",
      "0.8136221 0.8581820 0.7825896 0.7653610 0.5682025 0.8657516 0.6991604 0.6905305 \n",
      "      435       437       438       441       444       445       446       448 \n",
      "0.8743100 0.7316539 0.7448823 0.6308953 0.7855227 0.8735557 0.8458461 0.6892990 \n",
      "      449       452       453       455       457       461       462       463 \n",
      "0.5698472 0.7471626 0.8985194 0.6565643 0.6782299 0.8309048 0.7258155 0.6584658 \n",
      "      464       466       467       468       469       470       471       473 \n",
      "0.7243281 0.8663469 0.7391391 0.7277207 0.5291080 0.8338292 0.8037260 0.5830539 \n",
      "      476       477       478       479       480       481       482       483 \n",
      "0.6519343 0.7946864 0.6173477 0.8677807 0.8785618 0.7400312 0.7585631 0.6623299 \n",
      "      485       487       488       490       491       493       494       495 \n",
      "0.7719501 0.7218232 0.7358317 0.8123111 0.7225109 0.7008580 0.7636716 0.8626493 \n",
      "      497       499       501       503       504       505       507       508 \n",
      "0.7660825 0.7928043 0.6564433 0.8714489 0.6849393 0.6708064 0.7907763 0.7324631 \n",
      "      509       511       512       513       516       517       518       519 \n",
      "0.6582326 0.7362646 0.8886316 0.6971751 0.6491065 0.6638967 0.6351384 0.7267243 \n",
      "      520       521       523       524       525       526       528       530 \n",
      "0.4740733 0.5510683 0.7770598 0.6473314 0.7643541 0.5144676 0.7129379 0.6664971 \n",
      "      531       532       533       534       535       537       538       539 \n",
      "0.8439710 0.7488841 0.8171906 0.7173785 0.6793483 0.8105772 0.8442185 0.8528241 \n",
      "      540       541       543       544       546       547       548       549 \n",
      "0.7130436 0.7085475 0.7808689 0.6886121 0.7459014 0.5950316 0.7372552 0.6951396 \n",
      "      550       551       552       553       554       555       556       557 \n",
      "0.7843349 0.7393354 0.8388614 0.7273762 0.8126792 0.6830093 0.5630753 0.4286979 \n",
      "      558       559       560       561       562       563       564       567 \n",
      "0.7612724 0.6033051 0.7625024 0.7675340 0.6667540 0.3093626 0.7778905 0.5116194 \n",
      "      568       569       570       572       573       574       576       577 \n",
      "0.8801964 0.8120152 0.4662745 0.5514489 0.5959461 0.6392870 0.5351789 0.6573165 \n",
      "      579       580       581       583       584       585       587       588 \n",
      "0.5244193 0.4848377 0.6241565 0.6572404 0.6973435 0.8252581 0.7162733 0.6186368 \n",
      "      589       591       593       594       595       596       598       600 \n",
      "0.6553387 0.7230354 0.4828521 0.6641477 0.6142885 0.5314014 0.8758647 0.5412679 \n",
      "      602       603       606       607       608       609       610       611 \n",
      "0.6640232 0.9366841 0.6967611 0.8054483 0.8179156 0.4314538 0.6548645 0.7033106 \n",
      "      612       613       614       616       617       618       619       620 \n",
      "0.8031107 0.7422093 0.8135271 0.7339258 0.6038836 0.7011392 0.6203435 0.8361771 \n",
      "      621       623       624       625       626       627       628       629 \n",
      "0.7263274 0.9019205 0.7207427 0.8752879 0.8054407 0.7800886 0.8011250 0.8093035 \n",
      "      630       631       632       633       634       635       637       640 \n",
      "0.8610247 0.6929566 0.5972261 0.6302026 0.5610439 0.6370565 0.8805459 0.8463162 \n",
      "      641       643       646       647       648       649       650       651 \n",
      "0.6948718 0.7230051 0.5491611 0.7991590 0.4553748 0.6976768 0.7244684 0.7629745 \n",
      "      652       653       654       655       656       657       658       659 \n",
      "0.8240289 0.7939090 0.8030278 0.5089429 0.7741012 0.7251848 0.9141061 0.7909891 \n",
      "      660       661       662       664       666       669       670       671 \n",
      "0.7593881 0.7828059 0.7399258 0.7676182 0.8441077 0.6783361 0.7866837 0.6864415 \n",
      "      672       674       676       677       678       680       681       682 \n",
      "0.7219548 0.8404287 0.7658314 0.7321675 0.8098916 0.7716745 0.7139905 0.8017473 \n",
      "      684       685       687       689       690       691       692       693 \n",
      "0.8339355 0.6987350 0.7601595 0.7864591 0.6845608 0.2676152 0.7268554 0.7361466 \n",
      "      694       697       698       699       701       702       703       704 \n",
      "0.8029083 0.7730913 0.6549928 0.4637125 0.6503538 0.5749055 0.5948089 0.7869940 \n",
      "      705       707       708       709       711       712       713       714 \n",
      "0.9201951 0.5738243 0.6655440 0.4910725 0.8378746 0.7708028 0.5857586 0.8199451 \n",
      "      717       718       719       720       721       724       726       727 \n",
      "0.5964006 0.7945518 0.6833037 0.2511862 0.6081494 0.5205228 0.6324099 0.6843739 \n",
      "      728       730       731       732       733       734       735       737 \n",
      "0.7919197 0.3153645 0.6593914 0.3900659 0.6623669 0.6698341 0.6390313 0.8011826 \n",
      "      738       739       740       741       742       743       744       745 \n",
      "0.8038140 0.7750224 0.6619718 0.7474567 0.8805778 0.5797631 0.7931699 0.5496365 \n",
      "      746       747       748       750       751       752       753       754 \n",
      "0.7196003 0.6996281 0.7301034 0.7896275 0.5715988 0.7541880 0.7739605 0.6307137 \n",
      "      755       756       757       759       760       761       763       766 \n",
      "0.6238095 0.7484083 0.8426931 0.5752865 0.5482389 0.4640836 0.6508149 0.7100984 \n",
      "      767       769       770       771       772       773       774       775 \n",
      "0.4891041 0.7544621 0.6874046 0.6658126 0.7424671 0.8084230 0.4611680 0.5308451 \n",
      "      776       778       779       780       781       782       784       785 \n",
      "0.6122924 0.8346098 0.7287971 0.7633849 0.7032910 0.7857340 0.7486017 0.7319627 \n",
      "      787       788       789       791       792       797       799       800 \n",
      "0.7906584 0.8774461 0.6872955 0.6969080 0.6854545 0.7225718 0.6665727 0.7272801 \n",
      "      801       802       803       804       806       807       809       810 \n",
      "0.7042040 0.6845354 0.6332627 0.7303661 0.6272669 0.6872475 0.7148533 0.8045381 \n",
      "      811       812       813       814       817       818       819       820 \n",
      "0.3102833 0.3982189 0.6546702 0.6508852 0.8469917 0.7236410 0.7309863 0.6856219 \n",
      "      821       822       823       824       825       826       827       828 \n",
      "0.3809283 0.5246133 0.6789850 0.6497902 0.6600633 0.8581894 0.5045753 0.7940037 \n",
      "      829       830       832       833       834       835       836       838 \n",
      "0.7494942 0.3893356 0.5242395 0.4978505 0.5267110 0.7657964 0.5206757 0.7349336 \n",
      "      839       842       843       844       846       847       848       849 \n",
      "0.7265575 0.7104272 0.7397750 0.5707152 0.5858556 0.6237354 0.5163906 0.7188898 \n",
      "      850       853       854       855       856       857       858       859 \n",
      "0.6113601 0.6719623 0.7177155 0.7856322 0.5988687 0.6523810 0.5847908 0.5482675 \n",
      "      860       861       863       866       867       870       871       872 \n",
      "0.7665503 0.5359825 0.6975730 0.7808276 0.7128857 0.6334082 0.6665162 0.4830038 \n",
      "      874       875       877       879       880       881       882       884 \n",
      "0.4902978 0.6936795 0.5771904 0.7110008 0.7367364 0.6644476 0.7836808 0.6943635 \n",
      "      886       887       890       891       892       893       894       895 \n",
      "0.3989949 0.6322568 0.5036047 0.2756882 0.4576848 0.7113677 0.7296789 0.6917865 \n",
      "      896       897       898       900       901       904       906       907 \n",
      "0.6659029 0.7508986 0.5696907 0.7375435 0.6466084 0.7620579 0.6603144 0.7908682 \n",
      "      908       909       910       912       914       915       917       919 \n",
      "0.5649482 0.7551959 0.6226805 0.7781045 0.5805610 0.5925754 0.6789988 0.4174955 \n",
      "      920       922       923       924       926       928       929       930 \n",
      "0.5760863 0.8331509 0.6175466 0.7514584 0.6316741 0.6891647 0.4815745 0.3350407 \n",
      "      931       933       934       935       936       937       938       939 \n",
      "0.7108400 0.8302665 0.7790484 0.7885239 0.7088293 0.7579146 0.7500982 0.6193586 \n",
      "      940       941       942       943       944       946       947       949 \n",
      "0.6152831 0.5234079 0.8084340 0.8094573 0.5349974 0.7091273 0.7178113 0.7208446 \n",
      "      950       952       954       955       956       957       958       959 \n",
      "0.7660592 0.7349439 0.3528083 0.7248147 0.8452856 0.8505835 0.5390230 0.5706039 \n",
      "      961       962       963       965       967       968       969       970 \n",
      "0.8696482 0.6924429 0.5339040 0.7728650 0.5806531 0.6812506 0.4583388 0.7517202 \n",
      "      971       973       974       975       976       977       978       980 \n",
      "0.4867454 0.7471844 0.4098125 0.7423896 0.5595126 0.3504149 0.8742825 0.7593412 \n",
      "      982       983       984       985       986       987       988       989 \n",
      "0.6881646 0.4602419 0.6695901 0.4736985 0.8048703 0.7670099 0.6841683 0.6631841 \n",
      "      990       992       996       998       999      1000 \n",
      "0.5824143 0.6721850 0.5971216 0.5875903 0.8284898 0.5900067 \n",
      "        1         3         5         6         7         8         9        10 \n",
      "0.4639994 0.7136060 0.7251865 0.7098145 0.8135737 0.7068452 0.9490917 0.5617473 \n",
      "       11        12        13        14        16        17        19        20 \n",
      "0.7044501 0.5447432 0.7690522 0.3554833 0.7637496 0.7230036 0.6520652 0.9243818 \n",
      "       21        22        24        26        27        29        30        32 \n",
      "0.7429460 0.7793385 0.8316168 0.8313196 0.5135503 0.9117146 0.7795411 0.9061210 \n",
      "       33        35        36        38        39        40        41        43 \n",
      "0.8385630 0.5044384 0.7403224 0.9210412 0.9084697 0.8914806 0.9560863 0.9091360 \n",
      "       44        45        46        47        48        49        50        52 \n",
      "0.8586536 0.8600862 0.3604314 0.8824987 0.8971905 0.7854023 0.6925386 0.5613565 \n",
      "       53        54        55        56        57        58        59        60 \n",
      "0.9346300 0.9689478 0.9010384 0.8477198 0.9653292 0.4226123 0.9214407 0.8648952 \n",
      "       61        62        64        65        69        70        72        73 \n",
      "0.9629473 0.7377791 0.7762647 0.6645396 0.5972971 0.9179473 0.9331250 0.8674266 \n",
      "       74        75        78        79        80        82        83        84 \n",
      "0.9336065 0.8637776 0.9702361 0.5971581 0.9290607 0.9356587 0.6079351 0.8930662 \n",
      "       85        86        88        89        91        92        93        95 \n",
      "0.9761142 0.7084368 0.7448932 0.8883284 0.7013299 0.5779186 0.2885188 0.7522923 \n",
      "       96        98        99       100       102       103       104       105 \n",
      "0.9024896 0.3800163 0.5742219 0.6334077 0.4714736 0.7006332 0.5259927 0.9229806 \n",
      "      106       107       108       109       112       116       118       120 \n",
      "0.8498285 0.9098366 0.8294828 0.7858753 0.6971097 0.6777804 0.8811064 0.8867141 \n",
      "      122       123       124       126       127       128       130       131 \n",
      "0.8467704 0.6089792 0.9267830 0.8655367 0.8539458 0.9315972 0.9093216 0.6890604 \n",
      "      133       134       136       137       138       139       140       141 \n",
      "0.8458453 0.7714271 0.9277901 0.8816837 0.3312582 0.9139741 0.8567919 0.8612080 \n",
      "      142       143       144       145       149       150       152       153 \n",
      "0.9657727 0.6882858 0.9294852 0.8046311 0.9331431 0.7883602 0.8083715 0.8855239 \n",
      "      155       156       157       158       159       161       162       163 \n",
      "0.6648366 0.9027410 0.4922536 0.9136090 0.2225056 0.9210374 0.8505422 0.8400993 \n",
      "      164       165       166       168       169       170       171       174 \n",
      "0.9103262 0.6224661 0.9539460 0.9542514 0.9203452 0.9227104 0.5431168 0.8969216 \n",
      "      175       176       177       178       179       180       181       182 \n",
      "0.9240845 0.8497429 0.8822541 0.7784265 0.9152504 0.7935286 0.8088413 0.8576661 \n",
      "      184       185       186       187       188       189       190       191 \n",
      "0.8536681 0.9094008 0.8312386 0.9522838 0.8824653 0.6147198 0.6703574 0.8462558 \n",
      "      193       194       195       196       199       200       201       202 \n",
      "0.4604027 0.9110830 0.8787361 0.8936888 0.9325693 0.3567721 0.8650892 0.6125287 \n",
      "      204       206       207       208       209       210       211       214 \n",
      "0.3878706 0.6353495 0.5350046 0.6580327 0.1909233 0.9085003 0.8353661 0.9513847 \n",
      "      215       216       217       219       220       221       223       224 \n",
      "0.7533614 0.7358159 0.8791197 0.1557074 0.9524193 0.2267162 0.8908008 0.8614244 \n",
      "      225       227       228       229       230       231       233       234 \n",
      "0.6634023 0.5441748 0.8818224 0.9132721 0.2463307 0.8669520 0.7832927 0.3744178 \n",
      "      235       236       237       238       239       240       241       242 \n",
      "0.7288587 0.5851917 0.9041880 0.6757032 0.8365143 0.8027188 0.9556103 0.6030216 \n",
      "      243       246       247       248       250       251       252       253 \n",
      "0.8285426 0.8430472 0.9009416 0.8525146 0.8798071 0.7955833 0.6736269 0.9108323 \n",
      "      254       255       256       257       258       259       260       262 \n",
      "0.7673331 0.9173107 0.8110948 0.4754307 0.8620330 0.7114999 0.9744374 0.8924580 \n",
      "      263       264       265       267       268       269       271       272 \n",
      "0.6488045 0.8783311 0.8305030 0.7368035 0.8674756 0.8569895 0.9575244 0.8616392 \n",
      "      276       278       280       281       282       283       284       285 \n",
      "0.7517117 0.7727179 0.8171389 0.7174782 0.8022682 0.8288179 0.6186357 0.4959415 \n",
      "      286       287       288       289       290       291       292       294 \n",
      "0.9311678 0.8766528 0.8970842 0.9525470 0.8730128 0.9378733 0.8660287 0.6675250 \n",
      "      296       298       301       303       304       307       308       309 \n",
      "0.6837464 0.9184540 0.3735002 0.5290106 0.9572365 0.6652388 0.8249775 0.3492084 \n",
      "      311       313       314       315       316       317       318       319 \n",
      "0.9797883 0.9694763 0.4949310 0.6156775 0.9446849 0.7701884 0.7148336 0.9232005 \n",
      "      322       323       324       325       326       327       329       330 \n",
      "0.8620289 0.9062695 0.6603719 0.8973668 0.9406969 0.9098115 0.9409926 0.9021192 \n",
      "      333       334       335       337       338       339       340       341 \n",
      "0.9423319 0.9112599 0.9345443 0.8029038 0.8532168 0.8975315 0.9759416 0.9324927 \n",
      "      343       345       346       347       348       349       350       351 \n",
      "0.9471802 0.9649876 0.9098048 0.9498615 0.9333047 0.6873527 0.9332525 0.9432207 \n",
      "      352       353       354       356       357       358       359       360 \n",
      "0.9315654 0.8260170 0.6164280 0.8919097 0.6602177 0.9366686 0.6710266 0.9615680 \n",
      "      361       362       363       364       366       368       369       370 \n",
      "0.9644424 0.3919468 0.9526806 0.8665965 0.8021893 0.5245441 0.4986464 0.3426017 \n",
      "      371       372       375       376       377       378       379       380 \n",
      "0.3439799 0.9453546 0.9145015 0.4284533 0.5018237 0.5368612 0.6380863 0.9396007 \n",
      "      381       382       383       384       385       386       387       389 \n",
      "0.3276344 0.5158271 0.8419121 0.6501682 0.8786052 0.6925495 0.6696863 0.8217294 \n",
      "      390       392       393       394       396       398       402       403 \n",
      "0.5210625 0.9483645 0.9011810 0.9291437 0.6447849 0.8932207 0.8167175 0.6192933 \n",
      "      404       407       409       410       413       414       415       416 \n",
      "0.7190663 0.8618525 0.6965157 0.8262773 0.9602555 0.9147703 0.9631431 0.8786129 \n",
      "      417       418       419       420       421       422       423       424 \n",
      "0.9527104 0.9246396 0.9441131 0.7443407 0.8720462 0.8746339 0.8802376 0.9443689 \n",
      "      425       426       428       430       431       432       433       434 \n",
      "0.9442583 0.9576783 0.7784581 0.7565015 0.8734095 0.9652240 0.8069789 0.7911668 \n",
      "      435       437       438       441       444       445       446       448 \n",
      "0.9753788 0.9089660 0.8771496 0.8141620 0.9271076 0.9614588 0.9271156 0.8963846 \n",
      "      449       452       453       455       457       461       462       463 \n",
      "0.7705599 0.7610038 0.8108388 0.4835699 0.6210407 0.7336130 0.7255676 0.7626766 \n",
      "      464       466       467       468       469       470       471       473 \n",
      "0.5167419 0.8948877 0.7771167 0.8886467 0.5694501 0.9243141 0.9058053 0.8431339 \n",
      "      476       477       478       479       480       481       482       483 \n",
      "0.7579617 0.9012140 0.8338730 0.9484295 0.7473813 0.9115172 0.7239079 0.8396278 \n",
      "      485       487       488       490       491       493       494       495 \n",
      "0.9700367 0.8301857 0.7181782 0.9194060 0.8276814 0.4468720 0.9129828 0.8019197 \n",
      "      497       499       501       503       504       505       507       508 \n",
      "0.9227428 0.9240880 0.7593278 0.9596935 0.8586799 0.8794998 0.9534639 0.9396567 \n",
      "      509       511       512       513       516       517       518       519 \n",
      "0.8886942 0.9417767 0.9076804 0.5503373 0.5020098 0.6836563 0.8132363 0.8441340 \n",
      "      520       521       523       524       525       526       528       530 \n",
      "0.6813170 0.7241433 0.8888626 0.8304691 0.7858673 0.7666171 0.7019270 0.8795171 \n",
      "      531       532       533       534       535       537       538       539 \n",
      "0.9705989 0.9652594 0.7978402 0.7142918 0.3682149 0.9554647 0.9706676 0.9736526 \n",
      "      540       541       543       544       546       547       548       549 \n",
      "0.4113591 0.5292127 0.7199866 0.6068287 0.5854803 0.5943323 0.9336801 0.7519545 \n",
      "      550       551       552       553       554       555       556       557 \n",
      "0.9636590 0.7673684 0.9669473 0.8675780 0.9558564 0.6531737 0.7438840 0.1631722 \n",
      "      558       559       560       561       562       563       564       567 \n",
      "0.6839293 0.8348515 0.7484761 0.9019021 0.9301648 0.1552827 0.7598249 0.5325249 \n",
      "      568       569       570       572       573       574       576       577 \n",
      "0.9581250 0.9453091 0.8258419 0.8224750 0.4180363 0.4276961 0.3279748 0.4482607 \n",
      "      579       580       581       583       584       585       587       588 \n",
      "0.4994295 0.4462052 0.2905930 0.8435602 0.4957344 0.6419301 0.6448780 0.4129747 \n",
      "      589       591       593       594       595       596       598       600 \n",
      "0.5348404 0.5682118 0.2834447 0.1915134 0.1872600 0.4549382 0.8038132 0.4501657 \n",
      "      602       603       606       607       608       609       610       611 \n",
      "0.9282702 0.8575094 0.5562438 0.7520913 0.8036861 0.3476813 0.6008430 0.5327553 \n",
      "      612       613       614       616       617       618       619       620 \n",
      "0.5415265 0.5311656 0.5054680 0.5219796 0.4990181 0.6418999 0.2590673 0.8915202 \n",
      "      621       623       624       625       626       627       628       629 \n",
      "0.8573459 0.8415854 0.5556978 0.7646850 0.7472117 0.9326304 0.9666922 0.9454592 \n",
      "      630       631       632       633       634       635       637       640 \n",
      "0.6479465 0.5964740 0.4612323 0.4481898 0.3599255 0.5320313 0.8168417 0.7593820 \n",
      "      641       643       646       647       648       649       650       651 \n",
      "0.5578371 0.5780618 0.2172902 0.9375050 0.7717682 0.6283876 0.6668890 0.7207491 \n",
      "      652       653       654       655       656       657       658       659 \n",
      "0.8489926 0.8201830 0.9588662 0.5212287 0.7956772 0.6830378 0.8315285 0.7356324 \n",
      "      660       661       662       664       666       669       670       671 \n",
      "0.6045559 0.7318193 0.7712776 0.7287713 0.7393724 0.5926493 0.7326959 0.4979928 \n",
      "      672       674       676       677       678       680       681       682 \n",
      "0.6305516 0.8153697 0.8268339 0.6934618 0.7826544 0.7786960 0.3986338 0.7143768 \n",
      "      684       685       687       689       690       691       692       693 \n",
      "0.8936059 0.5441778 0.6177744 0.6858173 0.4632297 0.5404056 0.5359289 0.5838939 \n",
      "      694       697       698       699       701       702       703       704 \n",
      "0.8441645 0.7671956 0.8187182 0.5005718 0.5334939 0.5958787 0.4809932 0.6726291 \n",
      "      705       707       708       709       711       712       713       714 \n",
      "0.9147215 0.8220682 0.7854177 0.1323740 0.8938074 0.6317613 0.8985513 0.9602257 \n",
      "      717       718       719       720       721       724       726       727 \n",
      "0.4479253 0.9385188 0.6003913 0.1751736 0.4833292 0.8681072 0.5464375 0.9103505 \n",
      "      728       730       731       732       733       734       735       737 \n",
      "0.9082722 0.3377392 0.2420352 0.7064286 0.9448590 0.4447803 0.4229839 0.7504536 \n",
      "      738       739       740       741       742       743       744       745 \n",
      "0.9667625 0.7156675 0.3812403 0.9080112 0.9661103 0.4956295 0.9493432 0.8671228 \n",
      "      746       747       748       750       751       752       753       754 \n",
      "0.8695788 0.7292969 0.9100313 0.7183339 0.5157570 0.6089623 0.8880174 0.5060497 \n",
      "      755       756       757       759       760       761       763       766 \n",
      "0.6462029 0.4110867 0.7628208 0.4605742 0.4013821 0.4889108 0.7536668 0.9176024 \n",
      "      767       769       770       771       772       773       774       775 \n",
      "0.6327127 0.5495537 0.5939434 0.4777906 0.5035768 0.6021542 0.3725615 0.4650326 \n",
      "      776       778       779       780       781       782       784       785 \n",
      "0.2850213 0.6961602 0.7326450 0.8916666 0.7703423 0.4784544 0.4507400 0.2224258 \n",
      "      787       788       789       791       792       797       799       800 \n",
      "0.3510473 0.8714929 0.4236571 0.3276791 0.6104227 0.7546144 0.5619311 0.8231086 \n",
      "      801       802       803       804       806       807       809       810 \n",
      "0.5853638 0.6671509 0.5284666 0.7225489 0.3230811 0.3211793 0.5268379 0.3389848 \n",
      "      811       812       813       814       817       818       819       820 \n",
      "0.2299308 0.2143434 0.7544276 0.4298172 0.8354106 0.6707575 0.6418857 0.7879453 \n",
      "      821       822       823       824       825       826       827       828 \n",
      "0.5390350 0.3367690 0.3144993 0.4289191 0.3489797 0.7988984 0.3466192 0.7253870 \n",
      "      829       830       832       833       834       835       836       838 \n",
      "0.6999181 0.2759824 0.7515745 0.7423776 0.5747979 0.8286416 0.3860009 0.6462666 \n",
      "      839       842       843       844       846       847       848       849 \n",
      "0.6757621 0.6716673 0.9024312 0.4768428 0.7208595 0.4784306 0.1209257 0.5674890 \n",
      "      850       853       854       855       856       857       858       859 \n",
      "0.4809593 0.6232272 0.9193395 0.8417744 0.4691631 0.2208386 0.3029805 0.2699083 \n",
      "      860       861       863       866       867       870       871       872 \n",
      "0.4955153 0.7450768 0.5294446 0.9131629 0.5275608 0.4323808 0.4232866 0.7570283 \n",
      "      874       875       877       879       880       881       882       884 \n",
      "0.5301728 0.3905569 0.5664783 0.4207858 0.3925504 0.4881322 0.5950107 0.3441265 \n",
      "      886       887       890       891       892       893       894       895 \n",
      "0.4332343 0.3172369 0.3363058 0.2693480 0.4819806 0.7321264 0.4960044 0.5610682 \n",
      "      896       897       898       900       901       904       906       907 \n",
      "0.4891108 0.8830426 0.7552691 0.9282081 0.5606840 0.5760165 0.5592393 0.3417510 \n",
      "      908       909       910       912       914       915       917       919 \n",
      "0.4425659 0.5645439 0.2192831 0.7300450 0.1728099 0.2622740 0.4776299 0.1424748 \n",
      "      920       922       923       924       926       928       929       930 \n",
      "0.4569911 0.3222578 0.5018540 0.3478609 0.4213309 0.6067320 0.3531424 0.1379240 \n",
      "      931       933       934       935       936       937       938       939 \n",
      "0.7545270 0.6896365 0.6232331 0.7223750 0.6090444 0.6640895 0.7808029 0.4770310 \n",
      "      940       941       942       943       944       946       947       949 \n",
      "0.5195461 0.8414263 0.7518393 0.9166375 0.5328047 0.3526878 0.8803910 0.3746164 \n",
      "      950       952       954       955       956       957       958       959 \n",
      "0.9491389 0.9140050 0.6434260 0.3809717 0.7225471 0.6957884 0.4382601 0.2874590 \n",
      "      961       962       963       965       967       968       969       970 \n",
      "0.9517137 0.5646706 0.1541474 0.7161324 0.7175774 0.4805585 0.2950447 0.4071357 \n",
      "      971       973       974       975       976       977       978       980 \n",
      "0.6904348 0.7740217 0.1249244 0.4612217 0.1934065 0.1097991 0.6829353 0.5968464 \n",
      "      982       983       984       985       986       987       988       989 \n",
      "0.8432494 0.4330715 0.4517128 0.3295090 0.9395817 0.4677064 0.6144318 0.3632154 \n",
      "      990       992       996       998       999      1000 \n",
      "0.4776745 0.4975116 0.4352872 0.8999319 0.8160504 0.5133688 \n"
     ]
    }
   ],
   "source": [
    "# Exercise #3: \n",
    "\n",
    "#  Calculate the probabilities of  good credit risk for the customers in **train_credit** data based on **model1** and call it **model1_prob**\n",
    "\n",
    "#  Calculate the probabilities of  good credit risk for the customers in **train_credit** data based on **model2** and call it **model2_prob**\n",
    "\n",
    "#  Calculate the probabilities of  good credit risk for the customers in **train_credit** data based on **model3** and call it **model3_prob**\n",
    "\n",
    "set.seed(4230)\n",
    "\n",
    "# your code here\n",
    "\n",
    "model1_prob <- predict(model1, type = \"response\")\n",
    "model2_prob <- predict(model2, type = \"response\")\n",
    "model3_prob <- predict(model3, type = \"response\")\n",
    "print(model1_prob)\n",
    "print(model2_prob)\n",
    "print(model3_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0f47e14167fc388acda3ede21eefc42a",
     "grade": true,
     "grade_id": "cell-cf29d740c54d0fbf",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Passed!\"\n"
     ]
    }
   ],
   "source": [
    "# Test your code in here\n",
    "### BEGIN HIDDEN TEST\n",
    "\n",
    "test_that(\"Check the accuracy of calculated probabilities\", {\n",
    "    expect_equal( model1_prob[[1]],0.605092184658176)})\n",
    "\n",
    "test_that(\"Check the accuracy of calculated probabilities\", {\n",
    "    expect_equal( model2_prob[[33]],0.685393567269547)})\n",
    "\n",
    "test_that(\"Check the accuracy of calculated probabilities\", {\n",
    "    expect_equal( model3_prob[[10]],0.544743221429791)})\n",
    "\n",
    "print(\"Passed!\")\n",
    "\n",
    "### END HIDDEN TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16225e97bb8684be5c92d4c607902e65",
     "grade": false,
     "grade_id": "cell-177b5a16380fdfee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              \n",
       "model1_predict   0   1\n",
       "             0  80 125\n",
       "             1 145 400"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##  Now, we will label train data based on model1 results. \n",
    "# We will predict the **creditability** values in **train_credit** dataset by using the threshold level of 0.65.  \n",
    "#If the predicted probability takes the value higher than  0.65, we will label that customer as GOOD credit risk (creditability=1) and vice versa.\n",
    "\n",
    "model1_predict<-rep(0, nrow(train_credit))  # create  a matrix and assign everyone as 0 (Bad credit)\n",
    "model1_predict[model1_prob>0.65]= 1     # if the predicted probability is higher than 0.65,  label them as 1\n",
    "table(model1_predict,train_credit$creditability)  # create the confusion matrix. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "282b4f23f2a08d69cc0799dda6b76502",
     "grade": false,
     "grade_id": "cell-5e1225d82d292d58",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise  4\n",
    "\n",
    "Now,  your turn!  \n",
    "- By using **model2**, predict the **creditability** values in **train_credit** dataset by using the threshold level of 0.65 and name it **model2_predict**\n",
    "\n",
    "- By using **model3**, predict the **creditability** values in **train_credit** dataset by using the threshold level of 0.65 and name it **model3_predict**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f7caabe20dab9a58d62b5e3468a7340",
     "grade": false,
     "grade_id": "cell-f7ffbd05c8ebc4ba",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              \n",
       "model2_predict   0   1\n",
       "             0  87 116\n",
       "             1 138 409"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "              \n",
       "model3_predict   0   1\n",
       "             0 151 124\n",
       "             1  74 401"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exercise #4: \n",
    "\n",
    "# By using **model2**, predict the **creditability** values in **train_credit** dataset by using the threshold level of 0.65 and name it **model2_predict**\n",
    "\n",
    "# By using **model3**, predict the **creditability** values in **train_credit** dataset by using the threshold level of 0.65 and name it **model3_predict**\n",
    "\n",
    "set.seed(4230)\n",
    "\n",
    "# your code here\n",
    "model2_predict <-rep (0, nrow(train_credit))  # create  a matrix and assign everyone as 0 (Bad credit)\n",
    "model2_predict[model2_prob>0.65]= 1     # if the predicted probability is higher than 0.65,  label them as 1\n",
    "table(model2_predict,train_credit$creditability)\n",
    "\n",
    "model3_predict <-rep (0, nrow(train_credit))  # create  a matrix and assign everyone as 0 (Bad credit)\n",
    "model3_predict[model3_prob>0.65]= 1     # if the predicted probability is higher than 0.65,  label them as 1\n",
    "table(model3_predict,train_credit$creditability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5cc34d2733f1fcafe7f65213e761dbc",
     "grade": true,
     "grade_id": "cell-45ba693243857a40",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Passed!\"\n"
     ]
    }
   ],
   "source": [
    "# Test your code in here\n",
    "### BEGIN HIDDEN TEST\n",
    "\n",
    "test_that(\"Check the accuracy of predicted labels\", {\n",
    "    expect_equal(sum(model2_predict),547)})\n",
    "\n",
    "test_that(\"Check the accuracy of predicted labels\", {\n",
    "    expect_equal(sum(model3_predict),475)})\n",
    "\n",
    "print(\"Passed!\")\n",
    "\n",
    "### END HIDDEN TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4458eec46ce7da671a344aa1e1b2f348",
     "grade": false,
     "grade_id": "cell-9fd58522418cb0b9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Training Error Calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69d9ce44e23e4ff963df662984817b5f",
     "grade": false,
     "grade_id": "cell-8a8f2a2ec98523a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Based on model1, the classification error is:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.36"
      ],
      "text/latex": [
       "0.36"
      ],
      "text/markdown": [
       "0.36"
      ],
      "text/plain": [
       "[1] 0.36"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Based on model2, the classification error is:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.338666666666667"
      ],
      "text/latex": [
       "0.338666666666667"
      ],
      "text/markdown": [
       "0.338666666666667"
      ],
      "text/plain": [
       "[1] 0.3386667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Based on model3, the classification error is:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.264"
      ],
      "text/latex": [
       "0.264"
      ],
      "text/markdown": [
       "0.264"
      ],
      "text/plain": [
       "[1] 0.264"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  Now we can calculate the classification error rate. \n",
    "\n",
    "# create a function that will calculate the classification error\n",
    "class_error = function(true, predicted) {\n",
    "  mean(true != predicted)\n",
    "}\n",
    "\n",
    "print(\"Based on model1, the classification error is:\")\n",
    "class_error (train_credit$creditability,model1_predict)\n",
    "\n",
    "print(\"Based on model2, the classification error is:\")\n",
    "class_error (train_credit$creditability,model2_predict)\n",
    "\n",
    "\n",
    "print(\"Based on model3, the classification error is:\")\n",
    "class_error (train_credit$creditability,model3_predict)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f1f742321002e4318ff6a589c91a9ca0",
     "grade": false,
     "grade_id": "cell-67ce4ac63435030f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 5: \n",
    "According the the findings above, it looks like **model3** is doing a good job with the lowest classification error.  However, this finding (training error) can be misleading as we trained and tested the model on the same dataset. Training error can be overly optimistic. A more complex model, i.e **model3***, can perform relatively better than a simpler model in the training data. Hence, to better assess the performance of each model, we need to test them on an untouched (held out) data.  By this way, we get a better estimate of error rate of each model. \n",
    "\n",
    "-  Use **model1** to calculate the  probabilities of  good credit risk for the customers in **test_credit** dataset and name it **model1_test_prob**\n",
    "\n",
    "-  Use **model2** to calculate the  probabilities of  good credit risk for the customers in **test_credit** dataset and name it **model2_test_prob**\n",
    "\n",
    "-  Use **model3** to calculate the  probabilities of  good credit risk for the customers in **test_credit** dataset and name it **model3_test_prob**\n",
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4404c8f89270e330b3920db4cb4549de",
     "grade": false,
     "grade_id": "cell-f105ccfe5d889a6e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        2         4        15        18        23        25        28        31 \n",
      "0.7078173 0.7263977 0.6195893 0.6338763 0.6885161 0.7556755 0.7140927 0.7721921 \n",
      "       34        37        42        51        63        66        67        68 \n",
      "0.6548661 0.7202866 0.8258728 0.7827573 0.7721921 0.6479316 0.7979327 0.6548661 \n",
      "       71        76        77        81        87        90        94        97 \n",
      "0.6819293 0.6267605 0.7078173 0.7140927 0.7014617 0.7442236 0.8302208 0.6195893 \n",
      "      101       110       111       113       114       115       117       119 \n",
      "0.7324249 0.8386571 0.6617353 0.7078173 0.6409342 0.7014617 0.7014617 0.7979327 \n",
      "      121       125       129       132       135       146       147       148 \n",
      "0.6685370 0.7721921 0.6685370 0.7499933 0.8506726 0.7667754 0.7827573 0.7827573 \n",
      "      151       154       160       167       172       173       183       192 \n",
      "0.7442236 0.7202866 0.6548661 0.6338763 0.6267605 0.6195893 0.6338763 0.6479316 \n",
      "      197       198       203       205       212       213       218       222 \n",
      "0.7612697 0.6548661 0.7827573 0.6267605 0.6123656 0.6123656 0.6479316 0.7827573 \n",
      "      226       232       244       245       249       261       266       270 \n",
      "0.7499933 0.7263977 0.6819293 0.7612697 0.6338763 0.6479316 0.7324249 0.6950274 \n",
      "      273       274       275       277       279       293       295       297 \n",
      "0.6195893 0.8028118 0.7014617 0.8076016 0.6548661 0.6950274 0.6685370 0.6123656 \n",
      "      299       300       302       305       306       310       312       320 \n",
      "0.5904078 0.6950274 0.6338763 0.6548661 0.7383672 0.7263977 0.7202866 0.7014617 \n",
      "      321       328       331       332       336       342       344       355 \n",
      "0.7324249 0.7667754 0.6885161 0.6409342 0.6267605 0.6123656 0.6409342 0.7442236 \n",
      "      365       367       373       374       388       391       395       397 \n",
      "0.7140927 0.8214373 0.6617353 0.6479316 0.7499933 0.6885161 0.6338763 0.6885161 \n",
      "      399       400       401       405       406       408       411       412 \n",
      "0.7383672 0.6819293 0.6819293 0.6819293 0.7324249 0.6479316 0.8506726 0.6819293 \n",
      "      427       429       436       439       440       442       443       447 \n",
      "0.6819293 0.6195893 0.7078173 0.7499933 0.7324249 0.6409342 0.6685370 0.7078173 \n",
      "      450       451       454       456       458       459       460       465 \n",
      "0.8582661 0.8582661 0.6267605 0.6479316 0.7014617 0.6617353 0.6409342 0.6267605 \n",
      "      472       474       475       484       486       489       492       496 \n",
      "0.6338763 0.6885161 0.6752690 0.7827573 0.6479316 0.6409342 0.7442236 0.6409342 \n",
      "      498       500       502       506       510       514       515       522 \n",
      "0.7499933 0.7014617 0.6479316 0.6685370 0.7499933 0.6123656 0.5977719 0.6409342 \n",
      "      527       529       536       542       545       565       566       571 \n",
      "0.7324249 0.6819293 0.5977719 0.7383672 0.7442236 0.6409342 0.6195893 0.6409342 \n",
      "      575       578       582       586       590       592       597       599 \n",
      "0.7014617 0.7324249 0.6885161 0.6950274 0.6950274 0.7929639 0.7202866 0.7263977 \n",
      "      601       604       605       615       622       636       638       639 \n",
      "0.6409342 0.6050922 0.7383672 0.6617353 0.6479316 0.6123656 0.6548661 0.6267605 \n",
      "      642       644       645       663       665       667       668       673 \n",
      "0.6885161 0.8258728 0.8076016 0.5977719 0.7721921 0.7612697 0.6195893 0.6409342 \n",
      "      675       679       683       686       688       695       696       700 \n",
      "0.6409342 0.7014617 0.7078173 0.7324249 0.6617353 0.6267605 0.6050922 0.8214373 \n",
      "      706       710       715       716       722       723       725       729 \n",
      "0.7078173 0.6752690 0.6409342 0.6479316 0.7324249 0.6548661 0.6479316 0.6685370 \n",
      "      736       749       758       762       764       765       768       777 \n",
      "0.8427467 0.8619405 0.7775195 0.8545104 0.7442236 0.7721921 0.6819293 0.7324249 \n",
      "      783       786       790       793       794       795       796       798 \n",
      "0.8214373 0.7014617 0.6338763 0.8123022 0.8028118 0.8582661 0.8386571 0.6885161 \n",
      "      805       808       815       816       831       837       840       841 \n",
      "0.6195893 0.8467516 0.8854689 0.6123656 0.7442236 0.7078173 0.6409342 0.7324249 \n",
      "      845       851       852       862       864       865       868       869 \n",
      "0.7263977 0.6050922 0.6267605 0.6267605 0.6752690 0.6479316 0.6548661 0.6479316 \n",
      "      873       876       878       883       885       888       889       899 \n",
      "0.6123656 0.6548661 0.6752690 0.7442236 0.6548661 0.6338763 0.7014617 0.7078173 \n",
      "      902       903       905       911       913       916       918       921 \n",
      "0.6885161 0.8214373 0.6409342 0.8028118 0.6409342 0.6123656 0.6338763 0.6479316 \n",
      "      925       927       932       945       948       951       953       960 \n",
      "0.6409342 0.6752690 0.7140927 0.8214373 0.8028118 0.6267605 0.7263977 0.6123656 \n",
      "      964       966       972       979       981       991       993       994 \n",
      "0.6195893 0.6409342 0.7442236 0.6338763 0.6123656 0.6479316 0.7929639 0.6885161 \n",
      "      995       997 \n",
      "0.6548661 0.7612697 \n",
      "        2         4        15        18        23        25        28        31 \n",
      "0.7848706 0.7903790 0.6547322 0.6579532 0.5995449 0.8272801 0.8007329 0.7328347 \n",
      "       34        37        42        51        63        66        67        68 \n",
      "0.6553203 0.7891186 0.8309411 0.8464237 0.6098635 0.7606297 0.8465523 0.6472414 \n",
      "       71        76        77        81        87        90        94        97 \n",
      "0.6915874 0.7188059 0.7141053 0.7123030 0.6962686 0.7445216 0.8899338 0.6804595 \n",
      "      101       110       111       113       114       115       117       119 \n",
      "0.7179477 0.8848245 0.6992305 0.7722724 0.7236097 0.7017800 0.7852623 0.8678117 \n",
      "      121       125       129       132       135       146       147       148 \n",
      "0.5529624 0.8220075 0.7046491 0.7790817 0.9021038 0.7804448 0.7782593 0.8432293 \n",
      "      151       154       160       167       172       173       183       192 \n",
      "0.8161965 0.7069001 0.7140091 0.5482857 0.7068037 0.6995282 0.5258767 0.5395797 \n",
      "      197       198       203       205       212       213       218       222 \n",
      "0.8203189 0.6911163 0.8168535 0.6249068 0.5969307 0.6001785 0.7643756 0.7404537 \n",
      "      226       232       244       245       249       261       266       270 \n",
      "0.7538680 0.8283201 0.7272087 0.8474614 0.6828810 0.6960032 0.7383702 0.7498937 \n",
      "      273       274       275       277       279       293       295       297 \n",
      "0.7407980 0.8004685 0.6935775 0.8406176 0.6348377 0.7949986 0.7297362 0.7034162 \n",
      "      299       300       302       305       306       310       312       320 \n",
      "0.6745273 0.7268238 0.5982333 0.7535264 0.7420940 0.4451922 0.5472939 0.8045012 \n",
      "      321       328       331       332       336       342       344       355 \n",
      "0.8084503 0.8535760 0.6860458 0.6721744 0.6223074 0.5893294 0.7195536 0.8367428 \n",
      "      365       367       373       374       388       391       395       397 \n",
      "0.7784662 0.8291518 0.6045914 0.7669484 0.7087051 0.7327640 0.5948483 0.6471052 \n",
      "      399       400       401       405       406       408       411       412 \n",
      "0.8046748 0.7205397 0.6538352 0.6953322 0.5376824 0.7426993 0.8936903 0.6700401 \n",
      "      427       429       436       439       440       442       443       447 \n",
      "0.4785208 0.7003263 0.8070306 0.7296831 0.7543117 0.5845742 0.4390422 0.8125353 \n",
      "      450       451       454       456       458       459       460       465 \n",
      "0.9119466 0.9045230 0.5848706 0.3302559 0.7360563 0.6541137 0.7102383 0.6536872 \n",
      "      472       474       475       484       486       489       492       496 \n",
      "0.6366407 0.7391813 0.5640013 0.8182637 0.7267232 0.7211505 0.7983979 0.6412388 \n",
      "      498       500       502       506       510       514       515       522 \n",
      "0.7350480 0.7713426 0.6382479 0.7743246 0.8418353 0.5432037 0.6421079 0.4605694 \n",
      "      527       529       536       542       545       565       566       571 \n",
      "0.8045593 0.7203477 0.5895522 0.5592449 0.8437594 0.5706807 0.6165190 0.7191450 \n",
      "      575       578       582       586       590       592       597       599 \n",
      "0.5717693 0.8046640 0.6823890 0.6846751 0.7560921 0.8665368 0.7942037 0.6837815 \n",
      "      601       604       605       615       622       636       638       639 \n",
      "0.4873756 0.6859076 0.6565505 0.6155786 0.6027401 0.5939013 0.7223643 0.6272659 \n",
      "      642       644       645       663       665       667       668       673 \n",
      "0.5176101 0.6681775 0.8130394 0.6849213 0.8531755 0.8405231 0.6966281 0.6832210 \n",
      "      675       679       683       686       688       695       696       700 \n",
      "0.7246897 0.7474001 0.6893873 0.7350079 0.5528098 0.7024004 0.2274820 0.8293016 \n",
      "      706       710       715       716       722       723       725       729 \n",
      "0.7639364 0.4583617 0.7378098 0.5502467 0.7820771 0.6613273 0.6240486 0.4918596 \n",
      "      736       749       758       762       764       765       768       777 \n",
      "0.9070052 0.9132334 0.8398821 0.8770088 0.4550141 0.7699325 0.7201503 0.7491424 \n",
      "      783       786       790       793       794       795       796       798 \n",
      "0.6699030 0.6958459 0.6021442 0.7850557 0.8616867 0.8407001 0.7893141 0.7019070 \n",
      "      805       808       815       816       831       837       840       841 \n",
      "0.7027872 0.6161776 0.9187055 0.6485714 0.6541503 0.7252381 0.7187017 0.6557925 \n",
      "      845       851       852       862       864       865       868       869 \n",
      "0.6416245 0.4309915 0.7116673 0.4203280 0.7035533 0.4872470 0.4920015 0.7285400 \n",
      "      873       876       878       883       885       888       889       899 \n",
      "0.6607492 0.5865788 0.4520660 0.5304636 0.7033932 0.4809497 0.6954975 0.6704501 \n",
      "      902       903       905       911       913       916       918       921 \n",
      "0.7533161 0.7652209 0.6660159 0.7874293 0.4801519 0.6427095 0.6992770 0.6283403 \n",
      "      925       927       932       945       948       951       953       960 \n",
      "0.6430004 0.6612886 0.7851819 0.8766801 0.7363218 0.4381851 0.7490657 0.5944397 \n",
      "      964       966       972       979       981       991       993       994 \n",
      "0.6008341 0.3825950 0.6749486 0.7009797 0.6043453 0.6931719 0.7894865 0.7072212 \n",
      "      995       997 \n",
      "0.6929143 0.7600269 \n",
      "        2         4        15        18        23        25        28        31 \n",
      "0.7285720 0.7302580 0.6703991 0.5635514 0.4262297 0.7817540 0.8270033 0.8210739 \n",
      "       34        37        42        51        63        66        67        68 \n",
      "0.9024089 0.9402651 0.6001020 0.8964253 0.8996346 0.9246987 0.9251151 0.8843380 \n",
      "       71        76        77        81        87        90        94        97 \n",
      "0.8814088 0.8347927 0.9118103 0.8895927 0.9346316 0.7215966 0.8233962 0.6864758 \n",
      "      101       110       111       113       114       115       117       119 \n",
      "0.9483239 0.9803280 0.4723568 0.8504045 0.8590863 0.8017416 0.8670754 0.9725773 \n",
      "      121       125       129       132       135       146       147       148 \n",
      "0.3298702 0.9417023 0.8485473 0.8951299 0.9381900 0.9316375 0.6963028 0.7080379 \n",
      "      151       154       160       167       172       173       183       192 \n",
      "0.7073569 0.7245826 0.7572609 0.8636530 0.5014276 0.7275445 0.2156197 0.5450918 \n",
      "      197       198       203       205       212       213       218       222 \n",
      "0.9622718 0.8887878 0.7841034 0.6685495 0.2586955 0.2620331 0.8246205 0.8818036 \n",
      "      226       232       244       245       249       261       266       270 \n",
      "0.9464293 0.8412410 0.9446269 0.8646740 0.5888855 0.8164295 0.9109567 0.7316942 \n",
      "      273       274       275       277       279       293       295       297 \n",
      "0.9415657 0.9278977 0.8558461 0.9037208 0.4004926 0.9639263 0.9455561 0.8037543 \n",
      "      299       300       302       305       306       310       312       320 \n",
      "0.6475077 0.8543075 0.7360536 0.7603572 0.8663719 0.5754987 0.8835521 0.8475517 \n",
      "      321       328       331       332       336       342       344       355 \n",
      "0.9393145 0.9124220 0.9220574 0.8810761 0.3668441 0.1571429 0.8835295 0.9323245 \n",
      "      365       367       373       374       388       391       395       397 \n",
      "0.9357671 0.8443432 0.7257611 0.4856009 0.8467647 0.9166003 0.8071564 0.4758906 \n",
      "      399       400       401       405       406       408       411       412 \n",
      "0.9008366 0.7378868 0.4343801 0.8050869 0.5320443 0.6636129 0.9322505 0.8976824 \n",
      "      427       429       436       439       440       442       443       447 \n",
      "0.4907375 0.6120164 0.9492398 0.6496737 0.7324690 0.8549373 0.5535592 0.9680611 \n",
      "      450       451       454       456       458       459       460       465 \n",
      "0.8711913 0.8848175 0.4878526 0.3575479 0.7905608 0.6194532 0.8692045 0.6389049 \n",
      "      472       474       475       484       486       489       492       496 \n",
      "0.8914486 0.9364455 0.6043160 0.9632515 0.9069353 0.8650442 0.9402821 0.8727708 \n",
      "      498       500       502       506       510       514       515       522 \n",
      "0.9620136 0.6656065 0.9135730 0.9304384 0.7152239 0.3566597 0.9233157 0.6202824 \n",
      "      527       529       536       542       545       565       566       571 \n",
      "0.6149512 0.8744078 0.4919843 0.6641916 0.9705400 0.7941998 0.6273499 0.6980813 \n",
      "      575       578       582       586       590       592       597       599 \n",
      "0.4716965 0.6511704 0.3986186 0.6008826 0.5577553 0.8127129 0.6403107 0.4897658 \n",
      "      601       604       605       615       622       636       638       639 \n",
      "0.3020187 0.5384835 0.4516612 0.4131501 0.8600358 0.3401826 0.5888534 0.5216081 \n",
      "      642       644       645       663       665       667       668       673 \n",
      "0.2075689 0.4734250 0.9394479 0.6192276 0.8018432 0.6635642 0.6772859 0.5792800 \n",
      "      675       679       683       686       688       695       696       700 \n",
      "0.5579839 0.8925228 0.7892398 0.9434286 0.2044873 0.8734663 0.5213141 0.8171385 \n",
      "      706       710       715       716       722       723       725       729 \n",
      "0.9488600 0.7693133 0.8889497 0.6146473 0.8060897 0.8457307 0.4443228 0.8562184 \n",
      "      736       749       758       762       764       765       768       777 \n",
      "0.9516183 0.8769330 0.5140500 0.7143392 0.3368848 0.8897912 0.9173118 0.5838500 \n",
      "      783       786       790       793       794       795       796       798 \n",
      "0.5112341 0.8966383 0.5062807 0.1633142 0.5821084 0.8284874 0.5064017 0.6735031 \n",
      "      805       808       815       816       831       837       840       841 \n",
      "0.6310451 0.3479655 0.5972151 0.8711511 0.3213663 0.7624864 0.8949112 0.5723981 \n",
      "      845       851       852       862       864       865       868       869 \n",
      "0.6399448 0.3820186 0.2492016 0.1605147 0.9391045 0.4105080 0.2848668 0.5542777 \n",
      "      873       876       878       883       885       888       889       899 \n",
      "0.8171244 0.5785882 0.2229401 0.3091579 0.6055783 0.3528635 0.3665466 0.5858370 \n",
      "      902       903       905       911       913       916       918       921 \n",
      "0.5000297 0.6627596 0.2087472 0.5886324 0.4024302 0.4821833 0.3303071 0.2841684 \n",
      "      925       927       932       945       948       951       953       960 \n",
      "0.3857462 0.2267467 0.4843100 0.7380360 0.6928299 0.3913562 0.6321602 0.4209415 \n",
      "      964       966       972       979       981       991       993       994 \n",
      "0.2963764 0.1915036 0.3384770 0.7099141 0.4230966 0.4661395 0.6979041 0.5922601 \n",
      "      995       997 \n",
      "0.3399453 0.5741024 \n"
     ]
    }
   ],
   "source": [
    "# Exercise #5: \n",
    "\n",
    "#  Use **model1** to calculate the  probabilities of  good credit risk for the customers in **test_credit** dataset and name it **model1_test_prob**\n",
    "\n",
    "#  Use **model2** to calculate the  probabilities of  good credit risk for the customers in **test_credit** dataset and name it **model2_test_prob**\n",
    "\n",
    "#  Use **model3** to calculate the  probabilities of  good credit risk for the customers in **test_credit** dataset and name it **model3_test_prob**\n",
    "\n",
    "\n",
    "set.seed(4230)\n",
    "\n",
    "# your code here\n",
    "model1_test_prob<- predict(model1, test_credit, type=\"response\")\n",
    "model2_test_prob<- predict(model2, test_credit, type=\"response\")\n",
    "model3_test_prob<- predict(model3, test_credit, type=\"response\")\n",
    "\n",
    "print(model1_test_prob)\n",
    "print(model2_test_prob)\n",
    "print(model3_test_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ecf23063f56b41da6e597dc08dfb51f4",
     "grade": true,
     "grade_id": "cell-c03b8070f396caad",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Passed!\"\n"
     ]
    }
   ],
   "source": [
    "# Test your code in here\n",
    "### BEGIN HIDDEN TEST\n",
    "\n",
    "test_that(\"Check the accuracy of predicted labels\", {\n",
    "    expect_equal(max(model1_test_prob),0.885468929293488 )})\n",
    "\n",
    "\n",
    "\n",
    "test_that(\"Check the accuracy of predicted labels\", {\n",
    "    expect_equal(max(model2_test_prob),0.918705505426811 )})\n",
    "\n",
    "\n",
    "test_that(\"Check the accuracy of predicted labels\", {\n",
    "    expect_equal(max(model3_test_prob),0.980328014442461 )})\n",
    "\n",
    "\n",
    "print(\"Passed!\")\n",
    "\n",
    "### END HIDDEN TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2dd1cbc1af18208c37a18ab1a7832563",
     "grade": false,
     "grade_id": "cell-d90f627f6e3c918f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"classification error in test data based on model 1\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.42"
      ],
      "text/latex": [
       "0.42"
      ],
      "text/markdown": [
       "0.42"
      ],
      "text/plain": [
       "[1] 0.42"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"classification error in test data based on model 2\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.384"
      ],
      "text/latex": [
       "0.384"
      ],
      "text/markdown": [
       "0.384"
      ],
      "text/plain": [
       "[1] 0.384"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"classification error in test data based on model 3\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.268"
      ],
      "text/latex": [
       "0.268"
      ],
      "text/markdown": [
       "0.268"
      ],
      "text/plain": [
       "[1] 0.268"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classification <-function (true, predict,th) {\n",
    "    label<-rep(0, nrow(true))  # create  a matrix and assign everyone as 0 (Bad credit)\n",
    "    label[predict>th]= 1     # if the predicted probability is higher than th=0.65,  label them as 1\n",
    "    class_error(true$creditability,label)  # calculate the classification error  \n",
    "}\n",
    "\n",
    "print(\"classification error in test data based on model 1\")\n",
    "classification(test_credit,model1_test_prob,0.65)  #classification error in test data based on model 1\n",
    "\n",
    "\n",
    "print(\"classification error in test data based on model 2\")\n",
    "classification(test_credit,model2_test_prob,0.65)  #classification error in test data based on model 1\n",
    "\n",
    "print(\"classification error in test data based on model 3\")\n",
    "classification(test_credit,model3_test_prob,0.65)  #classification error in test data based on model 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 3 is the best model from train/test data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
